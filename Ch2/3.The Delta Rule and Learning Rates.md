## $\Delta$ 法则和学习率
在我们为训练快餐神经元推导精确算法之前，快速关注一下超参数(hyperparameters)。除了神经网络中定义的权重参数，学习算法还需要一组额外的参数来推进训练过程。这些所谓的超参数有一个就是学习率(learning rate)。

在实践中，沿着垂直轮廓方向移动的每一步，在重新计算新方向之前我们需要确定要走多远。这个距离取决于表面的陡峭程度。为什么？距离最小值越近，我们希望前进的步伐越小。接近最小值时我们是知道的，因为表面会平缓许多，所以可以用陡峭程度作为我们距离最小值多近的指示。然而，如果误差面相当圆润，训练可能花费较长的时间。因此，我们通常用学习率因子 $\epsilon$ 乘以梯度。选择学习率是个困难的问题(图2-4)，就像我们刚刚讨论的一样，如果学习率选择的太小，我们就要冒着训练过程时间漫长的风险。但是如果学习率选的过大，极有可能开始发散偏离最小值。在第三章，我们将学习各种使用自适应学习率的优化技术来自动化学习率选择过程。

![Convergence is difficult when our learning rate is too large](https://github.com/lucasbyAI/Fundamental_of_Deep_Learning_ZH/blob/master/images_folder/Fig2-4.png?raw=true)

图2-4 Convergence is difficult when our learning rate is too large

我们最后准备推导训练线性神经元的 $\Delta$ 法则。为了计算如何更新每个加权，我们求梯度，它本质上是误差函数对每个加权的偏导数。换句话说，我们想：

$$\begin{eqnarray}
\Delta w_k  &=&  - \epsilon \frac {\partial E}{\partial w_k} \\
&=& - \epsilon \frac {\partial}{\partial w_k}(\frac {1}{2} \sum_i(t^{(i)}-y^{(i)})^2 ) \\
&=& \sum_i \epsilon (t^{(i)}-y^{(i)}) \frac {\partial y_i}{\partial w_k} \\
&=& \sum_i \epsilon x_k^{(i)} (t^{(i)}-y^{(i)})
\end{eqnarray}$$

每次迭代时用这个方法更新加权，我们终于可以使用梯度下降了。
