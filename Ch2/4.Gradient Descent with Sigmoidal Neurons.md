## Sigmoid神经元的梯度下降
在本节及下一节，我们将训练神经元和使用非线性的神经网络。用sigmoid神经元作为模型，其他非线性神经元的推导留作读者的练习。为了简化，假设神经元不使用偏置项，尽管我们的分析很容易扩展到这种情况。我们仅假设偏置是传入连接上输入值总为1的加权。

让我们回顾一下逻辑神经元从输入计算输出值的机制：

$$ z=\sum_k w_k x_k $$

$$ y=\frac {1}{1+e^{-z}}$$

神经元计算输出的加权和，即 logit $z$。然后把logit放入输入函数计算最终输出 $y$。对我们来说非常幸运的是，这些函数都有非常好的导数，使得训练非常容易。对训练来说，要计算误差函数对权重的梯度。为了计算梯度，我们对logit求输入和权重的导数：

$$\frac{\partial z}{\partial w_k}=x_k$$

$$\frac{\partial z}{\partial x_k}=w_k$$

同样让人感到惊奇的是，如果把输出对logit的导数用输出表示也非常简单：

$$\begin{eqnarray}
\frac{\partial y}{\partial z}  &=&  \frac {e^{-z}}{(1+e^{-z})^2} \\
&=& \frac {1}{1+e^{-z}} \frac {e^{-z}}{1+e^{-z}} \\
&=& \frac {1}{1+e^{-z}} (1-\frac {1}{1+e^{-z}}) \\
&=& y(1-y)
\end{eqnarray}$$

用链式法则得到输出对每个加权的导数：

$$\frac{\partial y}{\partial w_k}=\frac{\partial y}{\partial z} \frac{\partial z}{\partial w_k}=x_ky(1-y) $$

把所有的导数放在一起，我们就能计算误差函数对每个加权的导数：

$$\frac{\partial E}{\partial w_k}=\sum_i\frac{\partial E}{\partial y^{(i)}} \frac{\partial y^{(i)}}{\partial w_k}=-\sum_i x_k^{(i)}y^{(i)}(1-y^{(i)})(t^{(i)}-y^{(i)}) $$

因此，修正权重的最后规则就变成：

$$\Delta w_k=\sum_i \epsilon x_k^{(i)}y^{(i)}(1-y^{(i)})(t^{(i)}-y^{(i)})$$

你可能注意到了，除了额外的解释sigmoid神经元逻辑成分的乘法项，新的修正法则就好像 $\Delta$法则。
