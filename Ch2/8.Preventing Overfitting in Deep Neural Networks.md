## 防止深度神经网络过拟合
在训练过程中，有几种推荐的技术用于防止过拟合。本节我们将详细讨论这些技术。

一种应对过拟合的方法叫做正则化(regularization)。正则化方法通过增加额外项惩罚大的权重来修正要最小化的目标函数。话句话说，我们改变目标函数使得它变为 $Error +
\lambda f(\theta)$，其中 $f(\theta)$ 随着 $\theta$ 要素的增大而增大，$\lambda$ 是正则化强度(另一个超参数)。为 $\lambda$ 选择的值决定了我们想多大程度防范过拟合。$\lambda=0$表明我们不采取任何措施防止过拟合的可能性。如果 $\lambda$ 过大，相比于在训练集上找到性能最好的参数，模型会优先保持 $\lambda$ 尽可能的小。因此，选择 $\lambda$ 是非常重要的任务，需要反复的试错。

机器学习中最常见的正则化类型是L2正则化 $^5$。把神经网络中所有权重幅度的平方加到误差函数里就是L2正则化。换言之，对神经网络中每个权重 $w$，在误差函数中加上 $\frac{1}{2} \lambda w^2$。L2正则化直观的解释就是猛烈地惩罚尖峰加权向量，倾向于扩散加权向量。这样做有个令人心动的特性，就是鼓励网络所有的输入都使用一点点，而不是只大量地使用部分输入。需要特别注意的是在梯度下降更新过程中，使用L2正则化最终意味着每个权值线性衰减到0。由于这个现象，L2正则化也常常被称作权值衰减。

我们可以用ConvNetJS可视化L2正则化的效果。类似于图2-10和图2-11，使用有2个输入，2个softmax输出，20个神经元的隐藏层的神经网络。用mini-batch梯度下降(batch大小为10)训练网络，正则化强度分别为0.01,0.1和1。可视化结果可以在图2-15中看到。

![A visualization of neural networks trained with regularization strengths of 0.01, 0.1, and 1 (in that order)](https://github.com/lucasbyAI/Fundamental_of_Deep_Learning_ZH/blob/master/images_folder/Fig2-15.png?raw=true)

图2-15 A visualization of neural networks trained with regularization strengths of 0.01, 0.1, and 1 (in that order)

另外一种常见的正则化类型是L1正则化。此处，我们对神经网络中的每个权重 $w$ 加入 $\lambda |w|$ 项。L1正则化有个有趣的属性，就是可以令权重向量在优化过程中变得稀疏(即非常接近于0)。换句话说，用L1正则化的神经元最终只用到最重要的输入的一小部分，对输入中的噪声很有抵抗力。相比之下，L2正则化的权重向量通常是分散的小数值。当你想准确了解哪个特征有助于决策时，L1正则化是非常有用的。如果这个层次的数据分析不是必需的，我们倾向于使用L2正则化，因此从经验上来看它的性能更好。

最大范数约束(max norm constraints)有类似的目标，尝试阻止 $\theta$ 变得太大，但是它实现起来更加直接 $^6$。最大范数约束对每个神经元的输入权重向量设定一个绝对上边界，使用映射的梯度下降实施约束。也就是说，任何时候梯度下降步进移动输入权重向量使得 ${||w||}_2 > c$，我们映射向量到半径为 $c$ 的球(以原点为中心)上。$c$ 的典型值是3和4。一个好的特性是参数向量不会失去控制(即使学习率太高了)，因为对权重的更新总是有限制的。

Dropout是一种不同类型的防止过拟合的方法，在深度神经网络中它已经成为最受青睐的防止过拟合的方法之一 $^7$。在训练时，只需要以一定的概率p(超参数)保持神经元激活就可以实现dropout，或者不想实现时设概率为0。直观地，这样强迫网络即使在缺失某些信息时也很准确，防止网络过于依赖任何一个(或者任何一小撮)神经元。更加数学化的表达是，通过有效地提供近似指数组合许多不同的神经网络架构的方法来防止过拟合。dropout的过程以绘画的形式表示在图2-16。

![Dropout sets each neuron in the network as inactive with some random probability during each minibatch of training](https://github.com/lucasbyAI/Fundamental_of_Deep_Learning_ZH/blob/master/images_folder/Fig2-16.png?raw=true)

图2-16 Dropout sets each neuron in the network as inactive with some random probability during each minibatch of training

直觉上很容易理解Dropout，但仍有复杂之处需要考虑。首先，我们希望测试时神经元输出等于训练时它们的期望输出。在测试时我们可以天真地缩小输出值来解决这个问题。例如，如果 $p=0.5$，为了与训练时有相同的期望输出，必须在测试时把输出值减半。这很容易理解，因为神经元的输出以 $1-p$ 的概率置为0。这意味着如果在dropout之前神经元的输出是 $x$，那么dropout后，期望输出将是 $E[output]=px+(1-p)\cdot0=px$。然而，这么幼稚的dropout实现是不受欢迎 ，因为它要求在测试时缩小神经元输出。测试时的性能对于模型评估是相当重要的，所以更可取的做法是使用反向dropout，即缩放发生在训练阶段而不是测试阶段。在反向dropout中，任何激活函数没有被压制的神经元的输出在传递到下一层之前除以p。用这种补救方法，$E[output]=p \cdot \frac{x}{p}+(1-p)\cdot0=x$，避免了在测试阶段随意缩小神经元的输出。

> 5 Tikhonov, Andrei Nikolaevich, and Vladlen Borisovich Glasko. “Use of the regularization method in nonlinear problems.” USSR Computational Mathematics and Mathematical Physics 5.3 (1965): 93-107.

> 6 Srebro, Nathan, Jason DM Rennie, and Tommi S. Jaakkola. “Maximum-Margin Matrix Factorization.” NIPS,
Vol. 17, 2004.

> 7 Srivastava, Nitish, et al. “Dropout: A Simple Way to Prevent Neural Networks from Overfitting.” Journal of Machine Learning Research 15.1 (2014): 1929-1958.
