## 用TensorFlow详述逻辑回归模型
既然我们已经开发了TensorFlow所有的基本概念，那么就创建一个简单的模型来处理MNIST数据集吧。正如你可能记得的那样，我们的目标是从28×28的黑白图像中识别手写数字。我们将要创建的第一个网络实现了被称为逻辑回归 $^{18}$ 的简单机器学习算法。

从大的方面来讲，逻辑回归是计算输入属于目标类别之一的概率的方法。在我们的场景里，我们将要计算一幅指定的输入图像是0,1，……，或者9的概率。模型用表示网络内连接加权的矩阵 $W$ 和对应偏置项的向量 $b$ 估计输入 $x$ 是否属于类别 $i$，用我们先前讨论的softmax表示：

$$
P(y=i|x)=softmax_i(Wx+b)=\frac{e^{W_ix+b_i}}{\sum_je^{W_jx+b_j}}
$$

我们的目标是学习到 $W$ 和 $b$ 的值，尽可能准确地有效区分输入图像。逻辑回归网络可以表示为图3-4描绘的那样(为了减少混乱，没有显示偏置项连接)。

![Interpreting logistic regression as a primitive neural network](https://github.com/lucasbyAI/Fundamental_of_Deep_Learning_ZH/blob/master/images_folder/Fig3-4.png?raw=true)

图3-4 Interpreting logistic regression as a primitive neural network

你将注意到逻辑回归的网络解释相当原始，没有任何隐藏层，这表明它学习复杂关系的能力是受限的！输出softmax大小为10，因为每个输入有10个可能的结果。此外，输入层的大小为784，每个输入神经元对应图像里的一个像素。正如我们将看到的，模型产生令人欣慰的进展，朝着正确分类数据集的方向迈进，但仍有很大的提升空间。在本章接下来的部分和第5章，我们将尽力显著提升模型准确率。但首先，让我们看一下如何用TensorFlow实现逻辑回归网络，这样就能够在计算机上训练它！

我们用四个阶段创建逻辑回归模型：
1. 推断(inference)：假定minibatch，生成输出类别上的概率分布

2. 损失(loss)：计算误差函数值(本例中采用交叉熵损失)

3. 训练(training)：负责计算模型参数的梯度和更新模型

4. 评估(evaluate)：决定模型的有效性

假定minibatch由表示MNIST图像的784维向量组成，用输入乘以表示输入层和输出层加权连接的矩阵的softmax表示逻辑回归。输出张量的每一行表示minibatch中每个相应数据采样在输出类别上的概率分布：

```
def inference(x):
     tf.constant_initializer(value=0)
     W = tf.get_variable("W", [784, 10],
                         initializer=init)
     b = tf.get_variable("b", [10],
                          initializer=init)
     output = tf.nn.softmax(tf.matmul(x, W) + b)
     return output
```

如果给出minibatch的正确标签，应该能够计算每个数据采样的平均误差。用下面在minibatch上计算交叉熵损失的代码段来实现平均误差计算：

```
def loss(output, y):
     dot_product = y * tf.log(output)

    # Reduction along axis 0 collapses each column into a
    # single value, whereas reduction along axis 1 collapses
    # each row into a single value. In general, reduction along
    # axis i collapses the ith dimension of a tensor to size 1.
     xentropy = -tf.reduce_sum(dot_product, reduction_indices=1)

     loss = tf.reduce_mean(xentropy)

     return loss
```

然后给出当前的损失，计算梯度并适当修正模型参数。使用TensorFlow内建的优化器让这些变得简单，优化器生成特殊的训练运算，这些运算在最小化时通过TensorFlow会话运行。注意到当我们生成训练运算时，也传递了表示已处理的minibatch数量的变量。每次训练运算执行时，这个step变量递增，因此我们可以跟踪训练进度：

```
def training(cost, global_step):
    optimizer = tf.train.GradientDescentOptimizer(learning_rate)
    train_op = optimizer.minimize(cost,global_step=global_step)
    return train_op
```

最后，我们把简单的计算子图放在一起在验证集或测试集上评估模型：

```
def evaluate(output, y):
    correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(y, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
    return accuracy
```

这就完成了逻辑回归模型的TensorFlow图创建。

> 18 Cox, David R. “The Regression Analysis of Binary Sequences.” Journal of the Royal Statistical Society. Series B(Methodological) (1958): 215-242.
