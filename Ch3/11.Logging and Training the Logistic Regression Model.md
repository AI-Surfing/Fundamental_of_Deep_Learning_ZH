## 日志记录和训练逻辑回归模型
既然已经有了所有主要的部件，我们开始把它们缝在一起。为了记录训练模型时的重要信息，我们记录几个摘要统计量。例如，用 $tf.scalar\_summary ^{19}$ 和 $tf.histogram\_summary ^{20}$ 命令记录每个小批量数据的代价，验证误差和参数分布。供参考，我们展示代价函数的scalar summary统计量：

```
def training(cost, global_step):
    tf.scalar_summary("cost", cost)
    optimizer = tf.train.GradientDescentOptimizer(learning_rate)
    train_op = optimizer.minimize(cost, global_step=global_step)
    return train_op
```

每个epoch，我们执行 $tf.merge\_all\_summaries ^{21}$ 收集已记录的所有摘要统计量，用 $tf.train.SummaryWriter$ 把日志写入硬盘。下一节，我们将描述如何用内建的TensorBoard工具可视化这些日志。

除了保存摘要统计量，也可以用 $tf.train.Saver$ 模型保存器保存模型参数。保存器默认保持最新的5个checkpoint，可以恢复它们给将来使用。

把所有代码放在一起，得到下面的Python脚本：

```
# Parameters
learning_rate = 0.01
training_epochs = 1000
batch_size = 100
display_step = 1

with tf.Graph().as_default():
    # mnist data image of shape 28*28=784
    x = tf.placeholder("float", [None, 784])

    # 0-9 digits recognition => 10 classes
    y = tf.placeholder("float", [None, 10])

    output = inference(x)

    cost = loss(output, y)

    global_step = tf.Variable(0, name='global_step', trainable=False)

    train_op = training(cost, global_step)

    eval_op = evaluate(output, y)

    summary_op = tf.merge_all_summaries()

    saver = tf.train.Saver()

    sess = tf.Session()

    summary_writer = tf.train.SummaryWriter("logistic_logs/",
                              graph_def=sess.graph_def)

    init_op = tf.initialize_all_variables()

    sess.run(init_op)

    # Training cycle
    for epoch in range(training_epochs):

        avg_cost = 0.
        total_batch = int(mnist.train.num_examples/batch_size)
        # Loop over all batches
        for i in range(total_batch):
        mbatch_x, mbatch_y = mnist.train.next_batch(batch_size)
        # Fit training using batch data
        feed_dict = {x : mbatch_x, y : mbatch_y}
        sess.run(train_op, feed_dict=feed_dict)
        # Compute average loss
        minibatch_cost = sess.run(cost,feed_dict=feed_dict)
        avg_cost += minibatch_cost/total_batch
    # Display logs per epoch step
    if epoch % display_step == 0:
        val_feed_dict = {
             x : mnist.validation.images,
             y : mnist.validation.labels
        }
        accuracy = sess.run(eval_op,
            feed_dict=val_feed_dict)

        print "Validation Error:", (1 - accuracy)

        summary_str = sess.run(summary_op,feed_dict=feed_dict)

        summary_writer.add_summary(summary_str,sess.run(global_step))

        saver.save(sess, "logistic_logs/model-checkpoint",
                   global_step=global_step)

print "Optimization Finished!"

test_feed_dict = {
     x : mnist.test.images,
     y : mnist.test.labels
}

accuracy = sess.run(eval_op, feed_dict=test_feed_dict)

print "Test Accuracy:", accuracy
```
在测试集上训练100个epoch，脚本得到的最终准确率为91.9%。这个准确率还不赖，但在本章最后一节我们用前馈神经网络处理这个问题时，会尽力得到更好的模型。

> 19 https://www.tensorflow.org/api_docs/python/tf/summary/scalar

> 20 https://www.tensorflow.org/api_docs/python/tf/summary/histogram

> 21 https://www.tensorflow.org/api_docs/python/tf/summary/merge_all
