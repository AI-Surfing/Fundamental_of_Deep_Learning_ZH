## 用TensorFlow创建多层模型
用逻辑回归模型，在MNIST数据集上能够达到8.1%的错误率。这看起来令人印象深刻，但对于高价值的实际应用却不是特别有用。例如，用我们的系统去读四位数(\$1000到\$9999)写出来的个人支票，将在近30%的支票上识别错误！为了创建一个更实际的MNIST数字读取器，让我们试试创建前馈网络来处理MNIST的挑战。

如图3-7所示，我们用两个隐藏层构建前馈模型，每个隐藏层有256个ReLU神经元：

![A feed-forward network powered by ReLU neurons with two hidden layers](https://github.com/lucasbyAI/Fundamental_of_Deep_Learning_ZH/blob/master/images_folder/Fig3-7.png?raw=true)

图3-7 A feed-forward network powered by ReLU neurons with two hidden layers

修改几处，我们可以复用逻辑回归例子的大部分代码：

```
def layer(input, weight_shape, bias_shape):
    weight_stddev = (2.0/weight_shape[0])**0.5
    w_init = tf.random_normal_initializer(stddev=weight_stddev)
    bias_init = tf.constant_initializer(value=0)
    W = tf.get_variable("W", weight_shape,
                         initializer=w_init)
    b = tf.get_variable("b", bias_shape,
                         initializer=bias_init)
    return tf.nn.relu(tf.matmul(input, W) + b)

def inference(x):
    with tf.variable_scope("hidden_1"):
        hidden_1 = layer(x, [784, 256], [256])

    with tf.variable_scope("hidden_2"):
        hidden_2 = layer(hidden_1, [256, 256], [256])

    with tf.variable_scope("output"):
        output = layer(hidden_2, [256, 10], [10])

    return output
```

新代码大部分是自解释的，但是初始化策略值得额外的描述。深度神经网络的性能很大程度上取决于参数的有效初始化。像下章将要描述的，深度神经网络的误差面有很多特性使得用随机梯度下降优化变得非常困难。随着模型层数(因此误差面的复杂度)的增加，这个问题变得愈发严重。精心设计的初始化是缓解这个问题的方法之一。

2015年He等发表的研究表明，对网络中的ReLU单元，加权的方差应该为 $\frac{2}{n_{in}}$，其中 $n_{in}$ 是进入神经元的输入的数量 $^{23}$。好奇的读者应该调查一下，当我们改变初始化策略会发生什么。例如，把 $tf.random\_normal\_initializer$ 改回在逻辑回归例子中用的 $tf.random\_uniform\_initializer$ 会严重损伤模型性能。

最后，为了更好一点的性能，我们在计算损失时进行softmax，而不是在推断阶段。这样就导致了下面的修正：

```
def loss(output, y):
    xentropy = tf.nn.softmax_cross_entropy_with_logits(output, y)
    loss = tf.reduce_mean(xentropy)
    return loss
```

运行这段程序300个epoch会比逻辑回归模型得到巨大的改进。模型运行得到98.2%的准确率，与我们第一次的尝试相比，每个数字的错误率几乎下降78%。

> 23 He, Kaiming, et al. “Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification.” Proceedings of the IEEE International Conference on Computer Vision. 2015.
