## TensorFlow中的会话
TensorFlow程序用会话与计算图交互 $^{13}$。TensorFlow会话负责创建初始图，可以用于适当地初始化所有变量和运行计算图。为了探索上述的每一个功能，让我们考虑下面这段简单的Python脚本：

```
import tensorflow as tf
from read_data import get_minibatch()

x = tf.placeholder(tf.float32, name="x", shape=[None, 784])
W = tf.Variable(tf.random_uniform([784, 10], -1, 1), name="W")
b = tf.Variable(tf.zeros([10]), name="biases")
output = tf.matmul(x, W) + b

init_op = tf.initialize_all_variables()

sess = tf.Session()
sess.run(init_op)
feed_dict = {"x" : get_minibatch()}
sess.run(output, feed_dict=feed_dict)
```

import声明后的头四行描述了最后实例化时会话创建的计算图。图3-2描绘了图(变量初始化运算)。然后按要求在sess.run(init_op)语句中用会话变量执行初始化运算，初始化变量。最后，可以再次调用sess.run运行子图。但是这次我们用feed_dict传想要计算的张量(或者一组张量)，它用必要的输入数据填充占位符。

![This is a an example of a simple computational graph in TensorFlow](https://github.com/lucasbyAI/Fundamental_of_Deep_Learning_ZH/blob/master/images_folder/Fig3-2.png?raw=true)

图3-2 This is a an example of a simple computational graph in TensorFlow

最后，sess.run接口也能用于训练网络。当用TensorFlow在MNIST上训练我们的第一个机器学习模型时，将会进一步讨论。但是sess.run一行代码究竟是如何实现如此多种多样的功能？答案在于底层计算图的强大表达力。所有这些功能用TensorFlow运算表示，被当做实参传给sess.run。所有sess.run要做的就是遍历计算图识别组成相关子图的所有依赖，确保所有属于同一个子图的占位符用feed_dict填充，然后返回子图(执行所有的中间运算)评估最初的实参。

现在对会话和如何运行它们有了综合的理解，我们接下来要探索创建和维护计算图的另外两个重要概念。

> 13 https://www.tensorflow.org/api_docs/python/tf/Session
