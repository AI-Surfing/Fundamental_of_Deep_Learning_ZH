## 详解变量作用域和共享变量
尽管目前我们还未遭遇这个问题，但创建复杂模型经常需要复用和共享在一处共同实例化的大量变量。不幸的是，如果我们不够小心，强求代码模块化和可读性会导致非预期的结果。让我们思考下面的例子：

```
def my_network(input):
    W_1 = tf.Variable(tf.random_uniform([784, 100], -1, 1),
                      name="W_1")
    b_1 = tf.Variable(tf.zeros([100]), name="biases_1")
    output_1 = tf.matmul(input, W_1) + b_1

    W_2 = tf.Variable(tf.random_uniform([100, 50], -1, 1),
                      name="W_2")
    b_2 = tf.Variable(tf.zeros([50]), name="biases_2")
    output_2 = tf.matmul(output_1, W_2) + b_2

    W_3 = tf.Variable(tf.random_uniform([50, 10], -1, 1),
                      name="W_3")
    b_3 = tf.Variable(tf.zeros([10]), name="biases_3")
    output_3 = tf.matmul(output_2, W_3) + b_3

    # printing names
    # 译者注：如果是Python3.x，请用print()代替
    print "Printing names of weight parameters"
    print W_1.name, W_2.name, W_3.name
    print "Printing names of bias parameters"
    print b_1.name, b_2.name, b_3.name
    return output_3
```

该网络包括六个变量描述的三层。因此如果想多次使用这个网络，我们倾向于把它封装成像my_network这样可以多次调用的简洁的函数。然而，当我们试图在两个不同的输入上用这个网络时，得到一些意外的结果：

```
In [1]: i_1 = tf.placeholder(tf.float32, [1000, 784],
                             name="i_1")

In [2]: my_network(i_1)
Printing names of weight parameters
W_1:0 W_2:0 W_3:0
Printing names of bias parameters
biases_1:0 biases_2:0 biases_3:0
Out[2]: <tensorflow.python.framework.ops.Tensor ...>

In [1]: i_2 = tf.placeholder(tf.float32, [1000, 784],
                             name="i_2")

In [2]: my_network(i_2)
Printing names of weight parameters
W_1_1:0 W_2_1:0 W_3_1:0
Printing names of bias parameters
biases_1_1:0 biases_2_1:0 biases_3_1:0
Out[2]: <tensorflow.python.framework.ops.Tensor ...>
```

如果仔细观察，第二次调用my_network并没有使用与第一次调用相同的变量(事实上是名字不同！)，而是生成了第二组变量！很多情况下，我们不想生成一个副本，相反地是想复用模型和它的变量。在本例中，我们不应该用tf.Variable。相反地，应该用一个更高级的利用TensorFlow变量作用域的命名体系。

TensorFlow变量作用域机制在很大程度上由两个函数控制：

tf.get_variable(<name>,<shape>,<initializer>)
检查使用该名字的变量是否存在，如果存在则重新使用，不存在就用shape和initializer生成它 $^{14}$。

tf.variable_scope(<scope_name>)
管理命令空间，确定tf.get_variable运算的作用域 $^{15}$。

让我们用TensorFlow变量作用域以更简洁的方式重写my_network。变量新名字被命名为“layer1/W”，“layer2/b”，“layer2/W”等：

```
def layer(input, weight_shape, bias_shape):
    weight_init = tf.random_uniform_initializer(minval=-1,
       maxval=1)
    bias_init = tf.constant_initializer(value=0)
    W = tf.get_variable("W", weight_shape,
                         initializer=weight_init)
    b = tf.get_variable("b", bias_shape,
                         initializer=bias_init)
    return tf.matmul(input, W) + b

def my_network(input):
     with tf.variable_scope("layer_1"):
          output_1 = layer(input, [784, 100], [100])

     with tf.variable_scope("layer_2"):
          output_2 = layer(output_1, [100, 50], [50])

     with tf.variable_scope("layer_3"):
          output_3 = layer(output_2, [50, 10], [10])
     return output_3
```
现在调用my_network两次，就像我们在上个代码块里做的。

```
In [1]: i_1 = tf.placeholder(tf.float32, [1000, 784],
                              name="i_1")

In [2]: my_network(i_1)
Out[2]: <tensorflow.python.framework.ops.Tensor ...>

In [1]: i_2 = tf.placeholder(tf.float32, [1000, 784],
                              name="i_2")

In [2]: my_network(i_2)
ValueError: Over-sharing: Variable layer_1/W already exists...
```

不像tf.Variable，tf.get_variable命令检查指定名字的变量是否已经实例化。默认是不允许共享(只是为了安全！)，但如果想在变量作用域内开启共享，可以显式说明：

```
with tf.variable_scope("shared_variables") as scope:
     i_1 = tf.placeholder(tf.float32, [1000, 784], name="i_1")
     my_network(i_1)
     scope.reuse_variables()
     i_2 = tf.placeholder(tf.float32, [1000, 784], name="i_2")
     my_network(i_2)
```

这使我们在允许变量共享的同时保持模块化。作为不错的副产品，命名体系也更简洁了。

> 14 https://www.tensorflow.org/api_docs/python/tf/get_variable

> 15 https://www.tensorflow.org/api_docs/python/tf/variable_scope
