## 在CPU和GPU上管理模型
如果我们非常渴望，TensorFlow允许使用多台计算设备创建和训练模型。支持的设备通常用下列组成格式的字符串ID表示：

"/cpu:0"  
机器的CPU

"/gpu:0"  
如果机器有GPU，这是第一个GPU

"/gpu:1"  
如果机器有GPU，这是第二个GPU

当TensorFlow运算同时有CPU和GPU核，且开启GPU使用，TensorFlow将自动选择使用GPU实现。为了检查计算图使用哪台设备，我们可以设置log_device_placement为True初始化TensorFlow会话：

```
sess = tf.Session(config=tf.ConfigProto(
                  log_device_placement=True))
```

如果要求使用指定设备，可以用tf.device $^{16}$ 选择合适的设备来实现。然而，如果已选的设备无法使用，将会抛出错误。如果已选设备不存在，我们希望TensorFlow寻找其他可使用的设备，像下面一样传allow_soft_placement标识给会话变量即可 $^{17}$：

```
with tf.device('/gpu:2'):
  a = tf.constant([1.0, 2.0, 3.0, 4.0], shape=[2, 2], name='a')
  b = tf.constant([1.0, 2.0], shape=[2, 1], name='b')
  c = tf.matmul(a, b)

sess = tf.Session(config=tf.ConfigProto(
       allow_soft_placement=True, log_device_placement=True))
sess.run(c)
```

TensorFlow也允许用图3-3所示的塔式方法横跨多个GPU创建模型。下面的代码是多GPU代码的一个例子：

```
c = []

for d in ['/gpu:0', '/gpu:1']:
  with tf.device(d):
    a = tf.constant([1.0, 2.0, 3.0, 4.0], shape=[2, 2],
                      name='a')
    b = tf.constant([1.0, 2.0], shape=[2, 1], name='b')
    c.append(tf.matmul(a, b))

with tf.device('/cpu:0'):
  sum = tf.add_n(c)

sess = tf.Session(config=tf.ConfigProto(
                   log_device_placement=True))

sess.run(sum)
```

![Building multi-GPU models in a tower-like fashion](https://github.com/lucasbyAI/Fundamental_of_Deep_Learning_ZH/blob/master/images_folder/Fig3-3.png?raw=true)

图3-3 Building multi-GPU models in a tower-like fashion

> 16 https://www.tensorflow.org/api_docs/python/tf/device

> 17 https://www.tensorflow.org/api_docs/python/tf/ConfigProto
