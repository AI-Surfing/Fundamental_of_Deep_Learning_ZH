## 梯度下降的挑战
尽管神经网络背后的基础思想已存在数十年，但直到最近基于神经网络的学习模型才成为主流。我们迷恋神经网络，用它的表达形式处理所有的事物，这是个通过创建多层网络解锁的技能。正如我们在前一章讨论的一样，深度神经网络能够破解从前视为难以对付的问题。然而，端到端训练深度神经网络充满了艰难的挑战，需要采取许多技术革新去克服，包括大规模的标记数据集(ImageNet,CIFAR等)，更好的硬件如GPU加速和多种算法的发现。

数年来，为了解决深度学习模型带来的复杂误差面，研究者致力于逐层的贪婪预训练 $^1$。这些时间密集型的策略在用小批量梯度下降收敛到最优参数设置之前每次为每层的模型参数试图寻找更加精确的初始化。然而，最近在优化方法上的突破使得我们能够以端到端的模式直接训练模型。

本章我们将讨论这些突破中的几个，接下来的几节主要聚焦在局部极小值和它们是否对成功训练深度模型产生障碍。在随后的的小节，我们将进一步探讨深度模型引起的非凸误差曲面，为什么小批量梯度下降不能胜任，以及现代非凸优化器如何克服这些陷阱。

> 1 Bengio, Yoshua, et al. “Greedy Layer-Wise Training of Deep Networks.” Advances in Neural Information Processing Systems 19 (2007): 153.
