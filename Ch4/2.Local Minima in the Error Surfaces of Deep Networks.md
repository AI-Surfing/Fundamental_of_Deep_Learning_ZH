## 深度网络误差曲面的局部极小值
在优化深度学习模型中，主要的挑战是我们不得不用极小局部信息去推断误差曲面的总体结构。这是一个很难的问题，因为在局部结构和总体结构之间很少存在对应关系。以下面的类推举例说明。

假设你是美国大陆上的一只蚂蚁，被随机扔在地图上，目标是找到这个曲面的最低点。你该怎么做？假设你所能看到的就是周围环境，这似乎是个难缠的问题。如果美国的地面是碗状的(或者从数学上来说是凸的)，而且学习率很智能，我们可以用梯度下降算法最终找到碗底。但是美国的地面极其复杂，就是说它不是个凸曲线，这意味着即使找到了山谷(局部极小值)，我们也不知道它是否是地图上的最低山谷(全局极小值)。在第二章，我们讨论了小批量版本的梯度下降在零梯度的欺骗性区域是如何帮助通过令人烦恼的误差曲线。但如我们在图4-1所看到的，甚至随机误差曲线也不能把我们从深度局部极小值中拯救出来。

![Mini-batch gradient descent may aid in escaping shallow local minima, but often fails when dealing with deep local minima, as shown](https://github.com/lucasbyAI/Fundamental_of_Deep_Learning_ZH/blob/master/images_folder/Fig4-1.png?raw=true)

图4-1 Mini-batch gradient descent may aid in escaping shallow local minima, but often fails when dealing with deep local minima, as shown

关键问题来了，理论上局部极小值抛出了严峻的问题。但实际上，深度网络误差曲线中的局部极小值有多普遍呢？在哪些情况下它们确实会给训练造成麻烦。下面两节，我们将剖开局部极小值的常见误解。
