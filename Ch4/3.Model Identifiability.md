## 模型可辨识性
局部极小值的第一个来源与通常被称为模型可辨识性的概念相关。关于深度神经网络的一个观察是它的误差曲面笃定有大量的局部极小值，有些情况下甚至是无穷多的局部极小值。这个观察是真实的主要有两个原因。

第一个原因是在全连接的前馈神经网络层内，任何神经元的重排在网络末端仍然会给出相同的最后输出。在图4-2用一个简单的三神经元层展示这个特性。因此，在 $n$ 个神经元的层内，有 $n!$ 种方式重排参数。对l层的深度网络，每层有n个神经元，我们总共有 $n!^l$ 种等价排列。

![Rearranging neurons in a layer of a neural network results in equivalent configurations due to symmetry](https://github.com/lucasbyAI/Fundamental_of_Deep_Learning_ZH/blob/master/images_folder/Fig4-2.png?raw=true)

图4-2 Rearranging neurons in a layer of a neural network results in equivalent configurations due to symmetry

除了神经元重排的对称性，不可分辨性以其他形式存在于某些种类的神经网络中。例如，对单个ReLU神经元导致的等价网络有无穷多种等价排列。因为ReLU使用分段线性函数，我们可以随意用任何不为零的常数 $k$ 乘以所有的输入加权，然后用 $\frac{1}{k}$ 缩放所有的输出加权而不改变网络的行为。我们把这个声明的证明留给积极的读者做练习。
