# 深度网络中虚假的局部极小值有多讨厌？
多年来尽管很少有证据，深度学习实践者们总是把训练深度网络时遇到的所有麻烦归咎于虚假的局部极小值。即使是今天，这仍然是个开放的问题，与全局极小值相比，高错误率的虚假局部极小值在实际深度网络中是否普遍存在。然而，多项近期研究似乎表明大多数局部极小值的错误率和泛化性与全局极小值非常相似。

一种方法是当训练深度神经网络时，我们设法通过绘制随时间变化的误差函数值来天真地处理这个问题。然而这种策略并不能提供关于误差曲线的足够信息，因为这很难看出误差曲线是否“颠簸”，或者说很难计算出我们应该向哪个方向移动。

为了更有效地分析这个问题，Goodfellow等(谷歌和斯坦福合作的研究小组)在2014年发表了一篇论文企图把这两个可能混淆的因素分离开 $^2$。他们巧妙地研究误差曲线在随机初始化的参数向量和成功的线性插值最终解之间发生了什么，而不是分析误差曲线随时间的变化。因此假定随机初始化参数向量 $\theta_i$ 和随机梯度下降(SGD)解 $\theta_f$，我们的目标是计算沿着线性插值 $\theta_{\alpha}=\alpha\cdot\theta_f+(1-\alpha)\cdot\theta_i$ 每一个点的误差函数。

换句话说，他们想研究即使我们知道向哪个方向移动，局部极小值是否阻碍基于梯度的搜索方法。研究表明对于大量不同类型神经元的实际网络，参数空间中随机初始点和随机梯度下降解之间的直接路径没有受到令人讨厌的局部极小值的困扰。

我们甚至可以用第三章创建的前馈ReLU网络自己证明这项研究。用训练原始前馈网络时保存的checkpoint文件，可以重新实例化推断和损失部分，同时也保存了一组原始图变量的指针便于var_list_opt(opt代表最优参数设置)将来使用：

```
# mnist data image of shape 28*28=784
x = tf.placeholder("float", [None, 784])
# 0-9 digits recognition => 10 classes
y = tf.placeholder("float", [None, 10])

sess = tf.Session()

with tf.variable_scope("mlp_model") as scope:
    output_opt = inference(x)
    cost_opt = loss(output_opt, y)
    saver = tf.train.Saver()
    scope.reuse_variables()
    var_list_opt = [
        "hidden_1/W",
        "hidden_1/b",
        "hidden_2/W",
        "hidden_2/b",
        "output/W",
        "output/b"
    ]
    var_list_opt = [tf.get_variable(v) for v in var_list_opt]
    saver.restore(sess, "mlp_logs/model-checkpoint-file")
```

类似地，可以复用成分构造器创建随机初始化网络，此处把变量保存在var_list_rand供下段程序使用：

```
with tf.variable_scope("mlp_init") as scope:
    output_rand = inference(x)
    cost_rand = loss(output_rand, y)
    scope.reuse_variables()
    var_list_rand = [
        "hidden_1/W",
        "hidden_1/b",
        "hidden_2/W",
        "hidden_2/b",
        "output/W",
        "output/b"
    ]
    var_list_rand = [tf.get_variable(v) for v in var_list_rand]
    init_op = tf.initialize_variables(var_list_rand)
    sess.run(init_op)
```

适当初始化这两个网络，就可以用混合参数alpah和beta构造线性插值：

```
with tf.variable_scope("mlp_inter") as scope:

   alpha = tf.placeholder("float", [1, 1])
   beta = 1 - alpha

   h1_W_inter = var_list_opt[0] * beta + var_list_rand[0] * alpha
   h1_b_inter = var_list_opt[1] * beta + var_list_rand[1] * alpha
   h2_W_inter = var_list_opt[2] * beta + var_list_rand[2] * alpha
   h2_b_inter = var_list_opt[3] * beta + var_list_rand[3] * alpha
   o_W_inter = var_list_opt[4] * beta + var_list_rand[4] * alpha
   o_b_inter = var_list_opt[5] * beta + var_list_rand[5] * alpha

   h1_inter = tf.nn.relu(tf.matmul(x, h1_W_inter) + h1_b_inter)
   h2_inter = tf.nn.relu(tf.matmul(h1_inter, h2_W_inter) + h2_b_inter)
   o_inter = tf.nn.relu(tf.matmul(h2_inter, o_W_inter) + o_b_inter)

   cost_inter = loss(o_inter, y)
```

最后，我们可以改变alpha的值来理解当来回移动随机初始点和最终SGD解之间的线段时，误差曲线是如何变化的：

```
import matplotlib.pyplot as plt

summary_writer = tf.train.SummaryWriter("linear_interp_logs/",
                                       graph_def=sess.graph_def)
summary_op = tf.merge_all_summaries()
results = []
for a in np.arange(-2, 2, 0.01):
    feed_dict = {
      x: mnist.test.images,
      y: mnist.test.labels,
      alpha: [[a]],
    }
    cost, summary_str = sess.run([cost_inter, summary_op],
                                 feed_dict=feed_dict)
    summary_writer.add_summary(summary_str, (a + 2)/0.01)
    results.append(cost)

plt.plot(np.arange(-2, 2, 0.01), results, 'ro')
plt.ylabel('Incurred Error')
plt.xlabel('Alpha')
plt.show()
```

这生成了图4-3，我们可以在图里审视自己。实际上，如果我们一遍遍运行这个实验，会发现没有真正棘手的局部极小值会使我们陷入困境。就是说，看起来梯度下降真正要对付的不是令人讨厌的局部极小值的存在，而是我们很难找到合适的方向移动。稍后我们会回过头来讨论这个想法。

![The cost function of a three-layer feed-forward network as we linearly interpolate on the line connecting a randomly initialized parameter vector and an SGD solution](https://github.com/lucasbyAI/Fundamental_of_Deep_Learning_ZH/blob/master/images_folder/Fig4-3.png?raw=true)

图4-3 The cost function of a three-layer feed-forward network as we linearly interpolate
on the line connecting a randomly initialized parameter vector and an SGD solution

> 2 Goodfellow, Ian J., Oriol Vinyals, and Andrew M. Saxe. “Qualitatively characterizing neural network optimization problems.” arXiv preprint arXiv:1412.6544 (2014).
