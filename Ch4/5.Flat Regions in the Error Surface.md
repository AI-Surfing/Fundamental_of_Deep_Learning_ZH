## 误差曲线的平坦区域
尽管看起来分析没有了麻烦的局部极小值，但当近似alpha=1时，我们注意到有块梯度近似为0的特殊平坦区域。这个点不是局部极小值，因此不太可能使我们完全陷入困境，但如果我们不幸遇到它，零梯度似乎会减慢学习。

更一般地，假定一个任意函数，梯度为0向量的点称为驻点(或临界点)。驻点有各种各样的形式，我们已经讨论了局部极小值。不难想象它们的同伴，没有对SGD造成太大问题的局部极大值。但有一些奇怪的驻点位于局部极小值和极大值之间，这些可能讨厌但不致命的平坦区域叫做鞍点。其实随着函数维度越来越多(即模型里有越来越多的参数)，鞍点以指数可能性多于局部极小值。让我们来一探究竟。

对于一维代价函数，驻点可用采取三种形式之一，如图4-4所示。不严格地，假设这三种配置中的每一种是等可能性的。这意味着一个随机一维函数的随机驻点，有三分之一的可能性是局部极小值。这说明如果我们总共有k个驻点，预计总共有 $\frac{k}{3}$ 个局部极小值。

![Analyzing a critical point along a single dimension](https://github.com/lucasbyAI/Fundamental_of_Deep_Learning_ZH/blob/master/images_folder/Fig4-4.png?raw=true)

图4-4 Analyzing a critical point along a single dimension

我们可以把这扩展到更高维的函数。考虑d维空间的代价函数，取任意一个驻点，算出这个点是局部极小值、局部极大值或者是鞍点比一维情况有点棘手。考虑图4-5的误差曲面，取决于如何切割曲面(从A到B或者从C到D)，驻点看起来既像极小值又像极大值。实际上它两者都不是，而是一种更复杂的鞍点。

![A saddle point over a two-dimensional error surface](https://github.com/lucasbyAI/Fundamental_of_Deep_Learning_ZH/blob/master/images_folder/Fig4-5.png?raw=true)

图4-5 A saddle point over a two-dimensional error surface

通常，在d维参数空间，我们可以在d个不同的轴切割驻点。在d个一维子空间的每一个里面驻点都表现为局部极小值，那么驻点只能是局部极小值。基于局部极小值在一维子空间可以采取三种不同形式之一的事实，我们意识到随机函数中一个随机驻点的概率是 $\frac{1}{3^d}$。这表明有k个驻点的随机函数期望有 $\frac{k}{3^d}$ 个局部极小值。换言之，随着参数空间维度的增加，局部极小值以指数减少。对这一话题更严格的处理不在本书的范围之内，但2014年Dauphin等人对此进行了更广泛的探索 $^{3}$。

那么这对于优化深层学习模式意味着什么呢？对随机梯度下降，这仍然不清楚。看起来误差曲面的这些平坦区段虽然讨厌但最终并没有阻止随机梯度下降收敛到好的答案。然而，它确实给试图直接解决梯度为零的点的方法带来了严重的问题。这已经成了深度学习模型某些二阶优化方法有效性的主要障碍，这一点我们稍后讨论。

> 3 Dauphin, Yann N., et al. “Identifying and attacking the saddle point problem in high-dimensional non-convex optimization.” Advances in Neural Information Processing Systems. 2014.
