## 当梯度点在错误的方向上
对深度网络误差曲线分析，优化深度网络最关键的挑战似乎是找到前进的正确轨迹。然而，当我们看局部极小值附近的误差曲线发生了什么时，一点也不奇怪这个主要的挑战。举个例子，考虑图4-6展示的定义在二维参数空间的误差曲面。

![Local information encoded by the gradient usually does not corroborate the global structure of the error surface](https://github.com/lucasbyAI/Fundamental_of_Deep_Learning_ZH/blob/master/images_folder/Fig4-6.png?raw=true)

图4-6 Local information encoded by the gradient usually does not corroborate the global structure of the error surface

回顾在第二章探讨过的轮廓，我们注意到梯度通常不是很好的良好轨迹的指示。特别的，仅当轮廓是完美的圆形时，梯度才总是指向局部极小值的方向。然而，如果轮廓极为椭圆(深度网络误差曲线的通常情形)，梯度可能不精确到偏离正确方向90°！

通过某种数学形式把分析扩展到任意数量的维度。对参数空间中每个加权 $w_i$，梯度计算 $\frac{\partial E}{\partial w_i}$ 的值或者当我们改变 $w_i$ 值时误差值如何变化。合并参数空间里所有权重，梯度给出最陡下降方向。然而，在这个方向上迈出重要一步的普遍问题是，当前进时脚下的梯度在变化！我们在图4-7展示了这个简单的事实。回到二维的例子，如果我们的轮廓是完美的圆形，在最陡下降方向上前进一大步，当我们移动时梯度不会改变方向。然而，对于高度椭圆的轮廓，情况并非如此。

![We show how the direction of the gradient changes as we move along the direction of steepest descent (as determined from a starting point). The gradient vectors are normalized to identical length to emphasize the change in direction of the gradient vector.](https://github.com/lucasbyAI/Fundamental_of_Deep_Learning_ZH/blob/master/images_folder/Fig4-7.png?raw=true)

图4-7 We show how the direction of the gradient changes as we move along the direction of steepest descent (as determined from a starting point). The gradient vectors are normalized to identical length to emphasize the change in direction of the gradient vector.

更普遍的，通过计算二阶导数我们可以量化当在特定方向上前进时脚下的梯度如何变化。明确地，我们想测量 $\frac{\partial(\partial E / \partial w_j)}{\partial w_i}$，它告诉我们当改变 $w_i$ 值时 $w_j$ 的梯度成分如何变化。我们可以把这个信息整理到一类特殊的矩阵-海森矩阵(H)。当描述在最陡下降方向前进时脚下梯度变化的误差曲面时，这个矩阵是病态的。

对于爱好数学的读者，我们稍微详细探讨一下纯粹用梯度下降海森矩阵是如何限制优化的。海森矩阵的某些属性(即它的实对称)允许我们在特定方向前进时有效地确定二阶导数(近似曲面曲率)。特别地，如果我们有单位向量 $d$，在那个方向上二级导数为 $d^Hd$。现在我们用泰勒级数展开的二阶近似理解沿 $x^{(i)}$ 估计的梯度向量 $g$ 从当前参数向量 $x^{(i)}$ 前进到新参数向量 $x$，误差函数到底发生了什么：

$$
E(x) \approx E(x^{(i)}) + (x - x^{(i)})^Tg + \frac{1}{2}(x - x^{(i)})^TH(x - x^{(i)})
$$

如果我们进一步规定将在梯度方向前进 $\epsilon$ 单元，表达式进一步简化为：

$$
E(x^{(i)} - \epsilon g) \approx E(x^{(i)}) - \epsilon g^Tg + \frac{1}{2}\epsilon^2g^THg
$$

这个表达式由三项构成：1）原参数向量的误差函数值，2）梯度幅度提供的误差上的改善，和3）包含海森矩阵表示的曲线曲率的修正项。

一般来说，我们可以用这些信息设计更好的优化算法。例如，我们甚至可以天真地用误差函数的二阶近似去确定每步的学习率，从而最大化误差函数的减少。然而，计算海森矩阵是项非常难的任务。在接下来的几节，我们将描述不用直接计算海森矩阵来解决病态矩阵的最优化突破。
