## 二阶方法简介
正如我们上节讨论的，计算海森矩阵是项计算量浩大的任务，动量提供了显著的加速而且不必担心计算的问题。然而，过去的几年人们研究了几种二阶方法来直接近似海森矩阵。为了完整起见，我们对这些方法作全面的阐述，但是更详细的内容超出了本书的范畴。

首先是共轭梯度法，尝试改进原始的最速下降法。在最速下降法中，计算梯度方向，然后沿着该方向线性搜索寻找最小值。跳到最小值，重新计算梯度决定下次线性搜索的方向。如图4-9所示，这种方法最终生成了大量的弯弯曲曲。因为每次在最陡方向上移动时，就撤销了其他方向上的一点进步。这个问题的补救措施是相对上一次选择的方向，在其共轭方向上移动，而不是在最陡方向上。用海森矩阵的间接近似选择共轭方向来线性联合梯度和上一次的方向。稍微改动一下，这个方法可以扩展到深度网络里的非凸误差曲线 $^6$。

![The method of steepest descent often zigzags; conjugate descent attempts to remedy this issue](https://github.com/lucasbyAI/Fundamental_of_Deep_Learning_ZH/blob/master/images_folder/Fig4-10.png?raw=true)

图4-10 The method of steepest descent often zigzags; conjugate descent attempts to remedy this issue

另外一种名为Broyden-Fletcher-Goldfarb-Shanno(BFGS)算法迭代计算海森矩阵的逆，用逆海森矩阵更加高效地优化参数向量 $^7$。在原始形式中，BFGS内存占用明显，但近期的工作已经产生了一个更加内存有效的版本L-BFGS $^8$。

总的来说，这些方法有一定的作用，二阶方法仍然是活跃的研究领域，但在从业者中并不流行。在撰写本书的时候，TensorFlow并不支持共轭梯度法或L-BFGS算法，尽管这些特征似乎在开发当中。

> 6 Moller, Martin Fodslette. "A Scaled Conjugate Gradient Algorithm for Fast Supervised Learning". Neural Networks 6.4 (1993):525-533.

> 7 Broyden,C.G. "A new method of solving nonlinear simultaneous equations". The Computer Journal 12.1 (1969):94-99

> 8 Bonnans, Joseph-Frederic, et al. Numerical Optimization:Theoretical and Practical Aspects. Springer Science & Business Media, 2006.
