## 学习率自适应
正如前面讨论的，训练深度网络的另一个主要挑战是选择适当的学习率。选择正确的学习率一直以来就是训练深度网络最令人烦恼的方面之一，因为它对网络性能有重要影响。学习率太小不能学习的足够快，但当我们接近局部极小值或病态区域时，学习太大将很难收敛。

现代深度网络优化的主要突破之一就是学习率自适应的出现。学习率自适应背后的基本思想是在学习过程中适当修正优化的学习率以达到更好的收敛特性。在下面的几节，我们将讨论AdaGrad，RMSProp和Adam这三种最流行的自适应学习率算法。

### AdaGrad-累积历史梯度
