## 传统计算机程序的局限性
对计算机来说，为什么“准确”是如此难解决的问题？传统的计算机程序被设计的擅长做两件事情：1)算术运算相当快；2）明确地遵循一组指令。所以如果要处理大量金融数据，那么你是幸运的。传统计算机程序能够处理这些，但是我们想做一些更有趣的事情，例如写一个程序自动识别某人的笔迹。图1-1将作为起点。

![](https://github.com/lucasbyAI/Fundamental_of_Deep_Learning_ZH/blob/master/images_folder/Fig1-1.png)    

尽管图1-1中的每个数字用稍微不同的方式书写，我们还是能够轻松地识别出第一行的每个数字为0，第二行的每个数字为1等等。让我们尝试写段计算机程序来破解这个任务，使用什么样的规则能够把某个数字与其他数字区分开？

好吧，我们先从简单的规则开始！例如，我们可以认为是0如果图像只有一个闭合的圆圈.图1-1中所有的例子似乎是满足这个规则的，但是这并不是充分条件。要是某人在写0时没有完全闭合圆圈该怎么办？正如图1-2所示，你怎么才能把凌乱的0和6区分开？

![](https://github.com/lucasbyAI/Fundamental_of_Deep_Learning_ZH/blob/master/images_folder/Fig1-2.png)   

你可以为圆圈起点和终点之间的距离设置一个截断值，但是我们并不清楚要把截断值设为多大。这个两难的窘境只是各种折磨的开始。我们如何区分3和5？或者区分4和9？我们可以通过仔细的观察和数月的试错来不断增加越来越多的规则或者特征，但是很明显这并不是一个非常容易的过程。

许多其他类型的问题也属于同一类问题：目标识别、 语言理解、自动翻译等。我们不知道要写什么样的程序，因为大脑工作的机制我们并不清楚。即使我们知道了大脑的工作机制，程序的复杂度也是叹为观止的。

