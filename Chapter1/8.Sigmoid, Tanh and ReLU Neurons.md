## Sigmoid, Tanh和ReLU神经元
在实践中主要使用的三种类型神经元在计算中引入了非线性。它们中的第一种是sigmoid神经元，它使用函数：
$$
f(z)=\frac{1}{1+e^{-z}}
$$
直观地，当logit非常小时，logistic神经元的输出接近于0。当logit非常大时，logistic神经元的输出接近于1。在这两种极端情况之间，如图1-11所示，神经元呈现 $S$ 型。

![The output of a sigmoid neuron as z varies](https://github.com/lucasbyAI/Fundamental_of_Deep_Learning_ZH/blob/master/images_folder/Fig1-11.png?raw=true)    

图1-11 The output of a sigmoid neuron as z varies

tanh神经元使用类似于 $S$ 型的非线性，但取值范围不再是0到1，tanh神经元的输出是-1到1。正如所期望的，它令 $f(z)=tanh(z)$。输出y和logit z 之间的变换关系如图1-12所示。但使用 $S$ 型非线性关系时，相对于sigmoid神经元，人们更青睐tanh神经元，因此它是以0为中心的。

![The output of a tanh neuron as z varies](https://github.com/lucasbyAI/Fundamental_of_Deep_Learning_ZH/blob/master/images_folder/Fig1-12.png?raw=true)    

图1-12 The output of a tanh neuron as z varies

受限线性单元(ReLU)神经元使用的是完全不同的非线性。它使用函数 $f(z)=max(0,z)$，产生曲棍球形状的响应特性，如图1-13所示。

![The output of a ReLU neuron as z varies](https://github.com/lucasbyAI/Fundamental_of_Deep_Learning_ZH/blob/master/images_folder/Fig1-13.png?raw=true)    

图1-12 The output of a ReLU neuron as z varies

尽管ReLU有一些缺陷 $^{[8]}$，但因为许多原因，最近它已成为很多任务选择的神经元(尤其在计算机视觉方面)。在第五章我们将讨论这些原因，以及客服潜在陷阱的策略。

> [8]: Nair, Vinod, and Geoffrey E. Hinton. “Rectified Linear Units Improve Restricted Boltzmann Machines” Pro‐ ceedings of the 27th International Conference on Machine Learning (ICML-10), 2010.
