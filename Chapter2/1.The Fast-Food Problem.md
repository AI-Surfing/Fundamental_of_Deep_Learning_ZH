## 快餐问题
我们开始理解用深度学习如何处理一些有趣的问题，但仍然还存在一个大问题：我们计算出的参数向量(神经网络中所有连接的权重)究竟如何？_训练_过程可以解决这个问题。在训练过程中，我们给神经网络大量的训练实例，迭代修正权重，最小化在训练实例上的误差。在训练足够的实例之后，我们期望神经网络非常高效地解决已经训练过的任务。

![](https://github.com/lucasbyAI/Fundamental_of_Deep_Learning_ZH/blob/master/images_folder/Fig2-1.png) 

图2-1 This is the neuron we want to train for the fast-food problem

我们继续前一章提到的包含线性神经元的例子。简要回顾一下，每天我们买份包含汉堡包、炸薯条和汽水的快餐，每种餐品买几份。我们可以预测快餐最终花费多少钱，但是每种餐品却没有价格。收银员告诉我们唯一的一件事就是快餐的总价，想要训练一个线性神经元解决这个问题，我们该怎么做？

一种方法是有技巧地地选取训练场景，一次只买汉堡包一种餐品，下一次只买炸薯条，最后一次只买汽水。一般来说，有技巧地选择训练案例是非常棒的主意。大量研究表明，通过构造优雅的训练集，能够让神经网络更加高效。使用这个方法唯一的问题是在实际情况中，它很少能让你解决问题。例如，在图像识别中，这个策略就没有清晰的类似关系。它仅仅是个不实际的解决方法。

反而，我们设法提出通常情况下奏效的解决方法。现在有大量的训练实例，使用图表中简单的公式我们能够计算出在 $i^{th}$ 训练实例上神经网络的输出。训练神经元使得我们得到可能的最优加权 - 加权最小化在训练案例上的预测误差。在这个例子中，我们要最小化使用到的所有训练案例的平方误差。正式地，如果我们知道 $i^{th}$ 训练实例的真实答案是 $t^{(i)}$，$y^{(i)}$ 是神经网络的预测值，要最小化的误差函数 $E$ 的值是：

$$E=\frac {1}{2} \sum_i (t^{(i)} - y^{(i)})^2$$

当模型对每个训练实例都做出理想的正确预测，平方误差为0。此外，$E$ 越接近0，我们的模型越好。因此，我们的目标就是选择参数向量 $\theta$ (模型中所有加权的值)使得 $E$ 尽可能地接近0。 

此刻你也许觉得奇怪，这个问题可以看做一组方程式，为什么我们还要用误差函数麻烦自己。别忘了，我们可有一组未知数(权重)和一组方程(每个训练实例一个方程)，如果训练实例不矛盾，误差自动就是0。

虽然那是聪明的观察，但不幸的是观察到的内容通用的效果不好。记住尽管此处我们使用线性神经元，但实际上线性神经元使用并不广泛，因此它们的学习能力有限制。在上一章结尾的时候谈到，我们开始使用sigmoid，tanh或者ReLU等非线性神经元，可以不再建立方程式组！显然，我们需要更好的策略来处理训练过程。