## 梯度下降
为了简化问题，可视化如何在所有训练实例上最小化平方误差。线性神经元仅有两个输入(因此只有两个加权，$w_1$ 和 $w_2$)。设想有个三维空间，它的水平维度对应加权 $w_1$ 和 $w_2$，垂直维度对应误差函数值E。在这个空间里，水平面上的点对应不同的加权值设置，它们对应的高度是产生的误差。考虑所有可能的加权上的误差，如图2-2所示，在三维空间中得到一个特别的二次项的碗状曲面。

![](https://github.com/lucasbyAI/Fundamental_of_Deep_Learning_ZH/blob/master/images_folder/Fig2-2.png)

图2-2 The quadratic error surface for a linear neuron

我们可以很方便地把这个曲线看做椭圆轮廓的集合，最小误差位于椭圆的中心。在这个例子里，我们生成了一个二维平面，它的维度对应两个加权，轮廓对应于生成相等E值的 $w_1$ 和 $w_2$ 组合。轮廓距离彼此越近，斜率越陡峭。事实上， 下降最陡峭的方向总是与轮廓垂直，用称为梯度的向量表示。

此刻我们形成了一种如何寻找最小化误差函数的加权值的高级策略。假设随机初始化网络加权将会发现我们自己在水平面上的某处，求当前位置的梯度，可以找到最陡下降的方向，在该方向上前进一步，发觉自己站在了比之前更加接近最小值的新位置上。在新位置上计算梯度重新求最陡下降方向，在新的位置上再向前进。图1-23描述了最陡下降，根据这个策略最终我们将会到达最小误差点。这就是著名的梯度下降算法，我们将用它来处理训练单个神经元问题和应对训练整个网络时更普遍的挑战 $^{[1]}$。

![](https://github.com/lucasbyAI/Fundamental_of_Deep_Learning_ZH/blob/master/images_folder/Fig2-3.png)

图2-3 Visulizing the error surface as a set of contours

> [1]: Rosenbloom, P. “The method of steepest descent.” Proceedings of Symposia in Applied Mathematics. Vol. 6. 1956.
