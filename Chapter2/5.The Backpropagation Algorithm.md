## 后向传播算法
现在我们最终准备处理训练多层神经网络(而不是单个神经元)问题。为了圆满完成这个任务，我们将使用后向传播方法，它是由David E. Rumelhart， Geoffrey E. Hinton 和 Ronald J. Williams 在1986 $^{[2]}$ 首次提出的。后向传播背后的思想是什么？我们不知道隐藏单元应该做什么，但我们能做的是当改变隐藏的活动状态时计算误差变化的有多快。从那里，当改变每个连接的加权时，我们能够计算出误差变化得多快。根本上来说，我们要找到最陡下降的路径，唯一掣肘的地方是要在相当高维的空间里计算。从对单个训练实例计算误差导数开始阐述。

每个隐藏单元可以影响很多输出单元，因此我们必须用信息量丰富的方法来联合多个独立的对误差的影响。我们的策略是一种动态规划，一旦得到了隐藏层的误差导数，就用它们计算下一层活动状态的误差导数。一旦找到隐藏单元活动状态的误差导数，就非常容易得到引入隐藏单元的误差导数。参见图2-5，为了讨论方便我们重新定义一些记号。

![](https://github.com/lucasbyAI/Fundamental_of_Deep_Learning_ZH/blob/master/images_folder/Fig2-5.png)

图2-5 Reference diagram for the derivation of the backpropagation algorithm

下标用于表示神经元的层，符号 $y$ 仍然表示神经元的活动状态。类似的，符号 $z$ 表示神经元的logit。我们看一下动态规划问题的基本案例，计算输出层的误差函数导数：

$$
E=\frac{1}{2} \sum_{j \in output}(t_j-y_j)^2
\Rightarrow
\frac{\partial E}{\partial y_j}=-(t_j-y_j)
$$

现在开始归纳，假设已知层 $j$ 的误差导数，要计算下一层，层 $i$ 的误差导数。为了这样做，必须积累层 $i$ 的神经元输出如何影响层 $j$ 每个神经元的logit的信息。logit对返回的下面一层输出数据的偏导数只是连接的权重 $w_{ij}$ ， 根据这个事实，可以按照下式来计算，

$$
\frac{\partial E}{\partial y_j}
=\sum_j \frac{\partial E}{\partial z_j} \frac{\partial z_j}{\partial y_i}
=\sum_j w_{ij} \frac{\partial E}{\partial z_j}
$$

进一步观察下面的式子：

$$
\frac{\partial E}{\partial z_j}
=\frac{\partial E}{\partial y_j} \frac{\partial y_j}{\partial z_j}
=y_j(1-y_j) \frac{\partial E}{\partial y_j}
$$

联合上面两个式子，根据层 $j$ 的误差导数把层 $i$ 的误差导数最终表示为：

$$
\frac{\partial E}{\partial y_i}
=\sum_j w_{ij} y_j(1-y_j) \frac{\partial E}{\partial y_j}
$$

一旦我们已经完成了整套动态规划流程，用偏导数(对隐藏单元活动的误差函数)适当地填充表格，然后确定关于加权误差如何变化。这就得到了在每次训练实例之后该如何修正权重：

$$
\frac{\partial E}{\partial w_{ij}}
=\frac{\partial z_j}{\partial w_{ij}} \frac{\partial E}{\partial z_j}
=y_i y_j(1-y_j)  \frac{\partial E}{\partial y_j}
$$

最后为了完成算法，像以前一样，只是把数据集中所有训练实例上的偏导数加起来，得到下面的修正公式：

$$
\Delta w_{ij}=-\sum_{k \in dataset} \epsilon y_i^{(k)} y_j^{(k)} (1-y_j^{(k)}) \frac {\partial E^{(k)}}{\partial y_j^{(k)}}
$$

这就完成了后向传播算法的描述。

> [2]：Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. “Learning representations by backpropagating
errors.” Cognitive Modeling 5.3 (1988): 1.
