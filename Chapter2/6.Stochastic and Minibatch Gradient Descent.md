## 随机和minibatch梯度下降
在"The Backpropagation Algorithm"一节已经描述的算法中，我们使用了一种被称为批量梯度下降(batch gradient descent)的梯度下降实现。批量梯度下降的思想是用整个数据集计算误差超平面，然后根据梯度选择最陡下降路径。对于简单的二次误差超平面，批量梯度下降效果很好。但大多数情况下，误差超平面可能更加复杂。让我们考虑一下图2-6展示的场景。

![Batch gradient descent is sensitive to saddle points, which can lead to premature convergence](https://github.com/lucasbyAI/Fundamental_of_Deep_Learning_ZH/blob/master/images_folder/Fig2-6.png)

图2-6 Batch gradient descent is sensitive to saddle points, which can lead to premature convergence

我们只有一个加权，用随机初始化和批量梯度下降寻找最优设置。然而误差超平面有一块平坦区域(在高维空间中也称为鞍点)，如果不幸的话，在做梯度下降时我们会发现自己被陷在这里。

另外一种有潜力的方法是随机梯度下降(stochastic gradient descent)，在每次迭代时只用一个实例来估计误差超平面。图2-7展示了这种方法，不再是一个静态的误差超平面，取而代之的是动态的误差超平面。因此，这种随机超平面上的下降极大改善了通过平坦区域的能力。

![The stochastic error surface fluctuates with respect to the batch error surface, enabling saddle point avoidance](https://github.com/lucasbyAI/Fundamental_of_Deep_Learning_ZH/blob/master/images_folder/Fig2-7.png)

图2-7 The stochastic error surface fluctuates with respect to the batch error surface, enabling saddle point avoidance

然而随机梯度下降的主要缺点是它每次只观察一个实例产生的误差，这也许并不是误差超平面的足够好的近似。而且，可能会使梯度下降耗费大量的时间。解决这个问题的一种方法是用mini-batch梯度下降。在mini-batch梯度下降中，每次迭代我们用总的数据集的某个子集来计算误差超平面(而不是只用一个实例)。这个子集叫做minibatch，除了学习率，minibatch大小是另外一个超参数。minibatch在批量梯度下降的效率与随机梯度下降提供的避免局部最小值之间达成平衡。在后向传播环境下，我们的加权更新步骤变为：

$$
\Delta w_{ij}=-\sum_{k \in minibatch} \epsilon y_i^{(k)} y_j^{(k)} (1-y_j^{(k)}) \frac{\partial E^{(k)}}{\partial y_j^{(k)}}
$$

这与上一节的推导是一致的，只不过不是在数据集所有实例上相加，而是在当前minibatch的所有实例上求和。
